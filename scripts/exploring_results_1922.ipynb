{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devin Short \n",
    "30 June 2023 \n",
    "shortda@uw.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial demonstrating how to explore data parsed out of the catalogues. The idea is to understand what kind of content we've generated, get a feel for how well our current tools are working, and try improving them. This notebook requires functions from `reporting.py`, currently living in the scripts directory of the github repository.\n",
    "\n",
    "Let's import some packages and set parameters we'll use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# If you're running this notebook from a directory that doesn't contain\n",
    "# reporting.py you'll either need to edit the following line to reflect\n",
    "# the relative location of the reporting module or copy reporting.py\n",
    "# into the current directory\n",
    "from reporting import histogram_strings_by_length\n",
    "\n",
    "# The year determines which issue of the catalogue we're looking at\n",
    "# across the entire notebook\n",
    "year = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Summarizing data with histograms</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to start is by looking at the number of characters in all entries parsed out of one year of the catalogue. Most good entries are going to have similar lengths because they display similar data. Extremely short entries are probably fragments created by OCR errors and some extremely long entries will be multiple entries the code we're using failed to split apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an overview by creating a histogram of the entry lengths. A histogram shows a series of bins on the bottom axis and the number of objects falling into each bin on the vertical axis. In this case I've created bins 5 characters wide, so each bar in the plot below shows the number of entries 0-5 characters long, 5-10 characters long, etc.\n",
    "\n",
    "I've also created parameters to examine \"underflow\" and \"overflow\" regions of the histogram. Underflow and overflow counts in a histogram refer to objects that are irrelevant for some reason. In many cases a histogram will have a single underflow or overflow bin that aggregates everything outside the region of interest, but I've included the full histogram here and shaded the outflow regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some idea of what's happening with different output data by examining different sets of entries. Try changing `clean_entries` to `full_entries` in the path above. Notice that the numbers of entries in each section of the histogram, reported above the plot, are different for the two data sets; clearly the existing code does more than identify short or long entries and drop them to transform \"full\" entries to \"clean\" entries. \n",
    "\n",
    "Looking at the shape of the histogram, you'll see there's a large peak around 50 characters, but then there are smaller peaks at about 90 characters and 140 characters. If you set `bins=range(400)` in the call to `histogram_strings_by_length` above, you'll see those secondary peaks are still present with higher resolution binning. How might we find out what causes that structure?\n",
    "\n",
    "Let's start by viewing some of the entry strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous cell several times, examining the output carefully each time. Are there clear differences in the contents of the entries from each peak? Can they be explained by reasonable features of a real data set or do you think the structure in the histogram is a result of bad parsing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we can draw three conclusions here:\n",
    "\n",
    "1. The largest peak is the easiest to explain: title-first entries, which only contain the book title, author name, price, and month of publication, are frequently about 50 characters long. There are of course a huge number of title-first entries much longer than that, and the other two peaks are sitting on top of a long tail from the title-first peak.\n",
    "2. The second peak looks like it consists mostly of author-first entries for single-author, single-title books.\n",
    "3. The third peak is the most complicated. I see books that have both multiple authors and long subtitles, as well as entries that have additional information like a series name, such as `(Langham bibelots, No. 10.)`, or a note about translation, for instance `English version by A. W. Verrall.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want an exercise to try working with the data yourself and to follow up on my interpretation above, figure out a way to histogram the shorter title-first entries separately from the longer author-first entries. If you don't know how to start, try this:\n",
    "\n",
    "1. Make a regular expression that searches for a sequence of capital letters. This won't be a perfect distinction, but the author-first entries usually have a publisher's name in all caps and the title-first entries don't.\n",
    "2. Use the regular expression to put entries with words in all caps in one pandas series and entries without words in all caps in another.\n",
    "3. Calculate the lengths of each element in each series and store them in two different series.\n",
    "4. Create histograms by using the method `.hist(bins=range(0, 400, 3))` on both series of lengths.\n",
    "5. Play around with the bin width (the third argument in the range function) and see how the features in each histogram change.\n",
    "\n",
    "You can refer to the last two code cells in this notebook to see an example of a similar process.\n",
    "\n",
    "If my interpretation above is mostly right then you should see a peak at 50 characters with a long tail to the right in one histogram and a smaller but very similarly shaped peak at about 90 characters in the other (I expect the second peak to have a shorter tail on the left as well). I haven't tried this, so if you do you should let me know what's going on with that third peak - is it present in both histograms? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Testing new methods</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries in the third peak reveal a flaw in our current approach to splitting the entries: some lines in the catalogue aren't part of an entry. These lines have the form `<author name> see <author name>`, suggesting a reader looking up one author might actually be looking for someone else. The code we're using assumes entries end with an OCR line ending in `12`. It doesn't handle the lines pointing from one author to another, so those lines usually get tacked onto the front of the following entry. The current code does sometimes parse these author-to-author lines as individual entries. It looks to me like this happens when they are the last OCR'd line on a page.\n",
    "\n",
    "I figured that out by repeatedly looking at samples generated by the next cell and then searching the Hathi Trust online PDF for the contents of author-to-author lines that appeared separate from a full entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can come up with a way to identify those author-to-author lines and separate them from the entries, you'll improve our data set. That's going to be pretty difficult though, so let's start with something easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at a sample of the longest entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A substantial number of long entries have a date that should mark a point to split entries that was missed for some reason. Reviewing a few samples in the previous cell, I see at least three cases that need to be handled:\n",
    "1. OCR mistakes, like `I2` or `1z` or `iz` rather than `12`\n",
    "2. Books published in 1911 and listed in this catalogue (our code assumes `12` is part of the entry delimiter)\n",
    "3. Entries with apparently well-formed 1912 dates that the current code failed on. For example, entry `14814` appears to be three entries separated by two 1912 dates that were skipped over for reasons I don't understand yet. Run the next cell to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two cases are relatively easy to handle. Let's start by finding the regular expressions our code uses to split entries apart and build on that to improve the current process. That code is documented in `Parsing_ECB_1912_wi23.ipynb`, currently living in the `scripts` directory of the github repository. In the section \"Splitting the Text into Entries\" in that notebook, I find the following process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\u04101.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'А1.'.encode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'APPENDIX LEARNED SOCIETIES, PRINTING CLUBS &c., WITH LISTS OF THEIR PUBLICATIONS, 1922'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'APPENDIX LEARNED SOCIETIES, PRINTING CLUBS &c., WITH LISTS OF THEIR PUBLICATIONS, 1922'.encode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total entries: 16294\n"
     ]
    }
   ],
   "source": [
    "# ALL CODE IN THIS CELL IS ADAPTED FROM Parsing_ECB_1912_wi23.ipynb\n",
    "\n",
    "# read the Hathi Trust OCR into memory\n",
    "with open(f'../princeton_years/ecb_19{year}.txt', encoding='utf8') as f:\n",
    "    ocr_full_text = f.read()\n",
    "\n",
    "# Separate main text from the front matter of the catalogue (note the\n",
    "# string called ocr_main_text still has the back matter attached until\n",
    "# a later step)\n",
    "# Note python allows multiple assignment: the split operation below\n",
    "# creates a two-element list whose elements are then assigned to two\n",
    "# different variables\n",
    "ocr_front_matter, ocr_main_text = re.split('\\u0410\\n', ocr_full_text)\n",
    "\n",
    "appendix_pattern = (\n",
    "    r'LEARNED SOCIETIES, PRINTING CLUBS &c., WITH LISTS OF THEIR' \n",
    ")\n",
    "\n",
    "# Separate main text from the back matter of the catalogue\n",
    "ocr_main_text, ocr_back_matter = re.split(appendix_pattern, ocr_main_text)\n",
    "\n",
    "# Make a regular expression to capture headers at the top of each\n",
    "# catalogue page\n",
    "header_capital_letters = r\"^(?:[A-Z\\-\\'\\sÈ]+)\"\n",
    "header_pattern = r\"^#(?s:.*?){}(?s:.*?){}(?s:.*?){}$\".format(\n",
    "    header_capital_letters, header_capital_letters, header_capital_letters\n",
    ")\n",
    "\n",
    "# Split the main text into pages and strip the headers. According to\n",
    "# Parsing_ECB_1912_wi23.ipynb this method fails to remove six headers.\n",
    "pages = [\n",
    "    re.sub(header_pattern, '', page, flags=re.M)\n",
    "    for page in ocr_main_text.split('\\f')\n",
    "]\n",
    "\n",
    "# Now find all lines ending in 12 and insert a token to split on\n",
    "entries_by_page = [\n",
    "    re.sub(r'(\\W22\\.?$)', '\\\\1<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in pages\n",
    "]\n",
    "\n",
    "# Split on the token to create a list of lists where each element is\n",
    "# a list of entries on an individual page\n",
    "entries_by_page = [\n",
    "    re.split(r'<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in entries_by_page\n",
    "]\n",
    "\n",
    "print(f'total entries: {sum([len(p) for p in entries_by_page])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test a new regular expression for splitting entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total entries: 324\n"
     ]
    }
   ],
   "source": [
    "# This cell assumes we're looking at the 1912 catalogue; try changing\n",
    "# this code (and the year parameter in the first code cell of the\n",
    "# notebook) to catch OCR mistakes for a different year.\n",
    "\n",
    "# create a list of possible values the OCR software might have created\n",
    "# when it ran into an 11 or 12 on the page\n",
    "ocr_interpretations_of_22 = [\n",
    "    '2s',\n",
    "    '2S',\n",
    "    's2',\n",
    "    'S2'\n",
    "]\n",
    "ocr_interpretations_of_19 = [\n",
    "    'i9',\n",
    "    'I9',\n",
    "    'l9'\n",
    "]\n",
    "terminators = ocr_interpretations_of_22\n",
    "\n",
    "# modify the existing regex with our new options\n",
    "entry_terminator_regex = r'(\\W({})\\.?$)'.format('|'.join(terminators))\n",
    "\n",
    "# get a new set of entries the same way as above, but using the new\n",
    "# regular expression\n",
    "new_entries_by_page = [\n",
    "    re.sub(entry_terminator_regex, '\\\\1<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in pages\n",
    "]\n",
    "new_entries_by_page = [\n",
    "    re.split(r'<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in new_entries_by_page\n",
    "]\n",
    "\n",
    "print(f'total entries: {sum([len(p) for p in new_entries_by_page])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we might have found 157 entries that weren't caught with the previous splitting expression, meaning we might have gained 314 entries that were either not present or invalid in the previous data set. Let's see if that's really what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of new entries: 311\n",
      "0      4113\n",
      "1      6609\n",
      "2      6108\n",
      "3      2889\n",
      "4      6111\n",
      "       ... \n",
      "318    5894\n",
      "319    6148\n",
      "320    5932\n",
      "321    6097\n",
      "322    1744\n",
      "Length: 311, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# It can be much more efficient to do things like check if elements in\n",
    "# one sequence exist in another using pandas objects rather than the\n",
    "# standard python library, so I flatten the lists of lists of entries\n",
    "# into pandas series containing all the entries.\n",
    "old_entries = pd.Series(\n",
    "    [e for page in entries_by_page for e in page],\n",
    "    dtype=pd.StringDtype()\n",
    ")\n",
    "new_entries = pd.Series(\n",
    "    [e for page in new_entries_by_page for e in page],\n",
    "    dtype=pd.StringDtype()\n",
    ")\n",
    "\n",
    "# strip nonprinting characters and replace all newlines in each entry\n",
    "# with spaces\n",
    "old_entries = old_entries.str.strip()\n",
    "new_entries = new_entries.str.strip()\n",
    "old_entries = old_entries.str.replace('\\n', ' ')\n",
    "new_entries = new_entries.str.replace('\\n', ' ')\n",
    "\n",
    "# Get new entries that don't exist in the old set\n",
    "new_entries = new_entries.loc[~new_entries.isin(old_entries)]\n",
    "\n",
    "print(f'number of new entries: {len(new_entries)}')\n",
    "#print(new_entries)\n",
    "print(new_entries.map(len))\n",
    "# plot a histogram of lengths of the new entries\n",
    "fig2 = new_entries.map(len).hist(bins=range(0, 6000, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram we started with covers nearly 20,000 entries. Here we have only 300, but the plot generated in the previous cell already looks extremely similar to the one at the top of the document. This means we can probably assume the entries we've produced here are just as good as the rest of the entries we're already capturing. Probably! I haven't actually looked at the new entries in detail, but it's 0200 and I need to finish this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Wrapping up</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here was to demonstrate how to play around with the data and the code for our summer project. The general method I followed was something like the following:\n",
    "1. Get some kind of overview of the output from our current parsing operations. I went with the size of the strings but there are all kinds of ways to do this. You could think about features of strings that have words in all caps and compare them to strings that don't have capitalized words (publishers in the author-first entries are almost always capitalized). You could start by exploring the dataframes, where the strings have been parsed out into specific fields, rather than thinking about whole entries. Explore the results we have and find something you think is interesting. If you can't think of a way to use that feature to explore the data, run it by one of your colleagues and see what they think. Your team is always your best asset!\n",
    "2. Use that view of the data to characterize what's working and what isn't. In this case it was clear that unreasonably short or long strings were going to have some errors, so I was able to look at those entries to see where there might be problems in the existing process.\n",
    "3. Find out which part of the existing code is relevant for the feature you're interested in. This part is probably going to be hard if you haven't had a much experience with programming. If it isn't obvious how to proceed after you've had a look at some of the scripts, ask me or Anna how to do what you want to do.\n",
    "4. See if you can modify the code to get different results.\n",
    "5. Use the same overview process you started with to compare your new result with the old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devin Short \n",
    "30 June 2023 \n",
    "shortda@uw.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial demonstrating how to explore data parsed out of the catalogues. The idea is to understand what kind of content we've generated, get a feel for how well our current tools are working, and try improving them. This notebook requires functions from `reporting.py`, currently living in the scripts directory of the github repository.\n",
    "\n",
    "Let's import some packages and set parameters we'll use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# If you're running this notebook from a directory that doesn't contain\n",
    "# reporting.py you'll either need to edit the following line to reflect\n",
    "# the relative location of the reporting module or copy reporting.py\n",
    "# into the current directory\n",
    "from reporting import histogram_strings_by_length\n",
    "\n",
    "# The year determines which issue of the catalogue we're looking at\n",
    "# across the entire notebook\n",
    "year = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to start is by looking at the number of characters in all entries parsed out of one year of the catalogue. Most good entries are going to have similar lengths because they display similar data. Extremely short entries are probably fragments created by OCR errors and some extremely long entries will multiple entries the code we're using failed to split apart.\n",
    "\n",
    "We can get an overview by creating a histogram of the entry lengths. A histogram shows a series of bins on the bottom axis and the number of objects falling into each bin on the vertical axis. In this case I've created bins 5 characters wide, so each bar in the plot below shows the number of entries 0-5 characters long, 5-10 characters long, etc.\n",
    "\n",
    "I've also created parameters to examine \"underflow\" and \"overflow\" regions of the histogram. Underflow and overflow counts in a histogram refer to objects that are irrelevant for some reason. In many cases a histogram will have a single underflow or overflow bin that aggregates everything outside the region of interest, but I've included the full histogram here and shaded the outflow regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'../entries/clean_entries/entries_19{year}.csv'\n",
    "\n",
    "# Run this cell and then play around here by adding bins=20 to the\n",
    "# list of arguments in the call to histogram_strings_by_length below.\n",
    "# Then try bins=range(400).\n",
    "# You can change the outflow boundaries by adding underflow_lim or\n",
    "# overflow_lim to the list of arguments and setting them to integers.\n",
    "# The default values are underflow_lim=30, overflow_lim=300.\n",
    "hist_results = histogram_strings_by_length(\n",
    "    path,\n",
    "    show_over_under=True,\n",
    "    drop_nulls=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some idea of what's happening with different output data by examining different sets of entries. Try changing `clean_entries` to `full_entries` in the path above. Notice that the numbers of entries in each section of the histogram, reported above the plot, are different for the two data sets; clearly the existing code does more than identify short or long entries and drop them to transform \"full\" entries to \"clean\" entries. \n",
    "\n",
    "Looking at the shape of the histogram, you'll see there's a large peak around 50 characters, but then there are smaller peaks at about 90 characters and 140 characters. If you set `bins=range(400)` in the call to `histogram_strings_by_length` above, you'll see those secondary peaks are still present with higher resolution binning. How might we find out what causes that structure?\n",
    "\n",
    "Let's start by viewing some of the entry strings (but first change the path in the previous cell back to `clean_entries` and execute it again if you changed the path at some point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2859      Career of Beauty Darling, Wyllarde (D.) 6s. Νου, 12\n",
      "10573        Lord's Day, Austin (G. B.) is. 6d. net.. July 12\n",
      "13404    Pentateuchal studies, Wiener (H. M.) 6s, net Νου. 12\n",
      "2557      Bushranger's secret, Clarke (Mrs. H.) 25... Sep. 12\n",
      "937        Autobiography, Johnson (I. C.) IS, 6d. net June 12\n",
      "13752      Poemis, Donne (J.) (Grierson) 18s, net. ...Oct. 12\n",
      "5764         Fairy book, English, Rhys (E.) 65....... Oct. 12\n",
      "13147      Paper makers, Directory of. 25. 6d. net .. Apr. 12\n",
      "4090     Creative revelation, Simpson (J. G.) 25. net July 12\n",
      "15735     Sign of the star, Williams (E.) 28. 6d. net Sep. 12\n",
      "15830        Sinner in Israel, Costello (P.) 6d.. ... June 12\n",
      "14176        Public House Trus., Park (A. F.) 6)..... July 12\n",
      "18674     Wives and daughters, Gaskell (Mrs.) 5s. net Oct. 12\n",
      "6726       Geometry, Intro. to, Taylor (E. O.) is. 6d. May 12\n",
      "7069     Gospels : Synoptic problem, Buckley (E. R.). Feb. 12\n",
      "14875       Rose in bloom, Alcott (L. M.) is. net ....Oct. 12\n",
      "3020      Change in the village, Bourne (G.) 58. net..Feb. 12\n",
      "6544      Furniture repairing, Taylor (C. S.) is, net Feb. 12\n",
      "6003        Fir Tree Farm, Everett-Green (1.) 6d. ....Oct. 12\n",
      "5698       Expedition of Captain Flick, Hume (F.) 6d. Apr. 12\n",
      "Name: 0, dtype: object \n",
      "\n",
      "\n",
      "18534         Williams (E. E. G.)-An Epitome of railway law. Cr. 8vo., 5s. net STEVENS & HAYNES, Mar. 12\n",
      "13318      Pedder (D. C.)— The Ministry of Poll Poorman, Cr. 8vo. 7* X5, pp. 310, 6s. E. ARNOLD, Mar, 12\n",
      "2328     Broun (A. F.)-Sylviculture in the tropics. 8vo. 8* X5), pp. 328, 8s. 6d. net MACMILLAN, Oct. 12\n",
      "11988         Morley (C. D.)—The Eighth sin. 12mo. pp. 60, swd., is. net........B. H. BLACKWELL, Dec. 12\n",
      "10342        Little pilgrim in the unseen (The). Pop. edit. 12mo., pp. 156, is, net ..MACMILLAN, Oct. 12\n",
      "8240     Hone (Percy F.)—Southern Rhodesia. Cheaper reissue. 8vo. 9 X 51, pp. 422, 6s, net BELL, Oct. 12\n",
      "11836    Moles worth (Mrs.) Mary. Reissue. Cr. 8vo., pp. 188, 2s, 6d. (Prize library) MACMILLAN, Nov. 12\n",
      "209         Ainsworth (William Harrison)-Old St. Paul's. Cr. 8vo., pp. 448, is, net.. ROUTLEDGE, Nov. 12\n",
      "15167       St. Nicholas. Vol. 39, Part 2, May to October, 1912. Ryl. 8vo., 6s, net...... WARNE, Oct. 12\n",
      "10792     McEwen (V.)--Knights of the Holy Eucharist. 16mo. 6° X4), pp. 76, 25. net GARDNER, D., Mar. 12\n",
      "1888       Bodily form of descendants of immigrants [to U.S.), Changes in, Boas (F.) 78. 6d. net Dec. 12\n",
      "206         Ainsworth (William Harrison)-Jack Sheppard. Cr. 8vo., pp. 438, is. net. . ROUTLEDGE, Nov. 12\n",
      "553      APPLIN Applin (Arthur)-Into Thy hands. Cr. Syo. 74 X5, pp. 320, 6s... ....F. V. WHITE, J une 12\n",
      "17167      Times.\"—Official index to The Times Monthly. (Annual sub. £5 58.) “ THE TIMES,\" Jan., &c., 12\n",
      "13515       Phillips (Sidney)—The Heavenward way. 12mo. 64 X43, pp. 96, Is. 6d. net GARDNER, D., Feb. 12\n",
      "16425        Stodart-Walker (A.)—The Well-intentioned. Cr. 8vo. 74 X 5, pp. 320, 6s. A. MELROSE, Mar. 12\n",
      "14628    Rhodes (Kathlyn)The Desert dreamers. Cr. 8vo. 71 X4, pp. 316, swd., is, net HUTCHINSON, Aug. 12\n",
      "2076          Bottomley (G.)-Chambers of imagery. Second ser, izmo., Swd. is. net .. E. MATHEWS, Apr. 12\n",
      "15588         Sheldon (Charles M.)- The High calling. Cr. 8vo. 74 X4, pp. 352, 6s. HIODDER & S., Feb. 12\n",
      "13222        Past-master of Monte Carlo (The). Illus. 8vo., pp. 48, Ios. 6d. net ......E. SEALE, Oct. 12\n",
      "Name: 0, dtype: object \n",
      "\n",
      "\n",
      "13231      Patch (Kate Whiting)—The Sensitive child, as revealed in some talks with a little boy. 2nd edit. 12mo., pp. 92, is, net ALLENSOX, Nov. 12\n",
      "14533       Reminiscences of a sunny clime. By M. E. W. 3rd edit., with added stories. Illus. Cr. 8vo. 7£.X44, pp. 110, 25. 61. net SIMPKIN, Nov. 12\n",
      "4268     Curtis (Charles 11.)- Annuals, hardy and half- hardy. 410., 8.1 x64, pp. 128, 1S. 6d. net. (Present day gardening ser.)......JACK, A pr. 12\n",
      "15597       Shelley (Percy Bysshe)--Poetical works. 2 vols. Fine-paper edit. 12mo. 64 X4, pp. 706, 568, ea. 25. net; Ithr. 33. net ..CHATTO, Sep. 12\n",
      "12657    O'Callaghan (M. A.)—Dairying in Australasia : farm and factory. Illus. Ryl. 8vo., 10 X6, pp. 804, 125. 6d. net AUSTRALIAN BOOK Co., Sep. 12\n",
      "9392        Kelly's Directory of Derbyshire, Nottinghamshire. Leicestershire and Rutland, 1912. Ryl. 8vo., 30s. ......KELLY'S ..DIRECTORIES, Sep. 12\n",
      "4248       Cunningham (D. J.)-Practical anatomy. Vol. 2. 5th edit., rev. by Arthur Robinson. Cr. 8vo. 71 X 48, pp. 646, 1os. 6d. net FROWDE, Nov. 12\n",
      "4486      Decon (C. A.) ed.-His will. New edit. 12mo. 38. 6d. net . HODDER & S. Decorative art, On the truth of, Fonseka (1. de) 28. 6d. net June 12\n",
      "2500       Burke (Edmund)--Speech on conciliation with the colonies Edit by E. J. Payne. Cr. 8vo., pp. 160, 28. 6d. (Clarendon Press) FROWDE, May 12\n",
      "7854         Heath (Sidney) and Haslehust (E. W.)-Exeter : described by S. H.; pictured by E. W. H. 4to. 9 x6, pp. 64, bds. 2s. net BLACKIE, Oct. 12\n",
      "9944       Leask (A. Ritchie)-Refrigerating machinery : its principles and management. 5th edit. Cr. 8vo. 71 X 44, pp. 310, 5s. net SIMPKIN, Jan, 12\n",
      "11590      Miles (Alfred H.) ed.—Where duty calls or danger : records of courage and adventure for girls. 8vo. 8 X 54, pp. 380, 5s. S. PAUL, Nov. 12\n",
      "14466       Reeve (Arthur B.)—The Black hand : the adventures of Craig Kennedy, scientific detective. Cr. 8vo. 71 X41, pp. 340, 25. net NASH, May 12\n",
      "9317         Jukes-Browne (A. J.)-The Student's handbook of stratigraphical geology. 2nd edit. 8vo. 8 X54, pp. 682, 125. net ...E. STANFORD, Mar. 12\n",
      "16806      No. 4, Autumn. Ea. 16 illus. Imp. 16mo. Tales of mystery and imagination, Poe (E. A.) Is., 71 X54, pp. 48, ea. 2}d. E. J. ARNOLD, Sep. 12\n",
      "1067        Ballads of Burma (Anecdotal and Analytical) ; by \" Oolay.” Illus. by T. Martin Jones. Cr. 8vo. 71 X54, pp. 132, 4s. net THACKER, Aug. 12\n",
      "15125        Salmon (The). By various authors. Cheaper re- issue. Cr. 8vo. 71 x5, pp. 278, 25. 6d. net (Fur, feather and fin ser.) LONGMANS, Jan. 12\n",
      "18461    Wilcox (Ella Wheeler)-An Ella Wheeler Wilcox treasury. 32mo. 34 X24, swd. 6d. net in envelope) (Langham bibelots, No. 8) SIEGLE, H., May 12\n",
      "14840    Rome, Religious life of ancient, Carter (J. B.) 8s. 6d, net.. ..Dec. II Rome, Society and politics in ancient, Abbott (F. F.) 6s.. ..May 12\n",
      "14272     Raabe (Wilhelm)-Else von der Tanne. Edit. with notes and vocabulary by S. J. Pease. Cr. 8vo., 3s. net (Oxford German ser.) FROWDE, Jan. 12\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get the clean entries from a csv file. Squeeze the result to convert\n",
    "# the single-column dataframe into a series.\n",
    "clean_entries = pd.read_csv(path, header=None).squeeze()\n",
    "\n",
    "# Drop null values\n",
    "clean_entries = clean_entries.dropna()\n",
    "\n",
    "# Get the length of each entry string\n",
    "lengths = clean_entries.map(len)\n",
    "\n",
    "# Get entry strings around the center of the largest peak\n",
    "first_peak = clean_entries.loc[(lengths > 47) & (lengths < 53)]\n",
    "\n",
    "# Get entries in the second peak\n",
    "second_peak = clean_entries.loc[(lengths > 89) & (lengths < 96)]\n",
    "\n",
    "# Get entries in the third peak\n",
    "third_peak = clean_entries.loc[(lengths > 134) & (lengths < 140)]\n",
    "\n",
    "# View a random sample of values in each peak\n",
    "print(first_peak.sample(20), '\\n\\n')\n",
    "print(second_peak.sample(20), '\\n\\n')\n",
    "print(third_peak.sample(20))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous cell several times, examining the output carefully each time. Are there clear differences in the contents of the entries from each peak? Can they be explained by reasonable features of a real data set or do you think the structure in the histogram is a result of bad parsing?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we can draw three conclusions here:\n",
    "\n",
    "1. The largest peak is the easiest to explain: title-first entries, which only contain the book title, author name, price, and month of publication, are frequently about 50 characters long. There are of course a huge number of title-first entries much longer than that, and the other two peaks are sitting on top of a long tail from the title-first peak.\n",
    "2. The second peak looks like it consists mostly of author-first entries for single-author, single-title books.\n",
    "3. The third peak is the most complicated. I see books that have both multiple authors and long subtitles, as well as entries that have additional information like a series name, such as `(Langham bibelots, No. 10.)`, or a note about translation, for instance `English version by A. W. Verrall.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries in the third peak reveal a flaw in our current approach to splitting the entries: some lines in the catalogue aren't part of an entry. These lines have the form `<author name> see <author name>`, suggesting a reader looking up one author might actually be looking for someone else. The code we're using assumes entries end with an OCR line ending in `12`. It doesn't handle the lines pointing from one author to another, so those lines usually get tacked onto the front of the following entry. The current code does sometimes parse these author-to-author lines as individual entries. It looks to me like this happens when they are the last OCR'd line on a page.\n",
    "\n",
    "I figured that out by repeatedly looking at samples generated by the next cell and then searching the Hathi Trust online PDF for the contents of author-to-author lines that appeared separate from a full entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries containing \" see \": 364\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7497                            Hall (C.) see Munroz (C. E.) and Hall. Hall (C.) see Routledge (J. J.) and Hall. 302, 6s.\n",
       "17189                       Tipton (P. J. Varley-) see Varley-Tipton. Tirol : Brenner Pass, Clare (C. L.) 6s, net Sep. 12\n",
       "15822                                 Singh (Bhawani) see Bhawani Singh. Singing bone, Freeman (R. A.) 2s, net....Feb. 12\n",
       "2944           Cavalcaselle (G. B.) see Crowe (J. A.) and Cavalcaselle. Cavalier of fortune, Lynn (E.) 35. 6d.....Oct. 12\n",
       "13338                           Peet (J. 0.) see Taylor (H.) and Peet. Peer's progress, Clouston (J. S.) is. net..Jan. 12\n",
       "17513                        Turquet (A.) see Perry (C. P.) and Turquet. Turquoise ring, Lemon (I.) is. 60. ......Nov. 12\n",
       "10289            Linton (Charles E. T. Stuart-) see Stuart-Linton. Lion-Heart, Strang (H.) and Stead (R.) is. 6d. Oct. 12\n",
       "322                              Amatis (L. Bartlett-) see Bartlett-Amatis. Amazing Duke, Magnay (Sir W.) 6d..... June 12\n",
       "18761             Wootton (William Ord) see Cahen (E.) and Wootton. Word and the world, Wakeford (J.) 35. 6d. net Sep. 12\n",
       "1231                         Base (D.) see Simon (W.) and Base. Baseball : Pitching in a pinch, Mathewson (C.) ...Oct. 12\n",
       "12612                         Nunn (T. Percy) see Barrett (E.) and Nunn. Nunnery, Truthi about a, Ayesha (M.) 6s, Mar. 12\n",
       "3988     Coulthurst (s. L.) see Mortimer (F. J.) and Coulthurst. Counsel assigned, Andrews (M. R. S.) 25. 6d, net Aug. 12\n",
       "5942                    Feuvre (Amy le) see Le Feuvre. Fever, Puerperal, &c., Statistics, Geddes (G.) 6s, net ... Aug. 12\n",
       "13146       Pape (F. A. G.) see Smith (H. H.) and Pape. Paper cutting and modelling, Tolson (J. E.) 25. 6d. net ..Mar. 12\n",
       "7381          Grove (H. M.) see Haenen (F. de) and Grove. Grove family of Halesowen, Davenport (J.) 78. 6d. net ..Mar. 12\n",
       "6301                  Fox-Reeve (Iris) see Allhusen (B.) and Fox-Reeve. Fox, Dale (T. F.) 25. 6d. net............ Jan. 12\n",
       "7556             Hands (A. P. M.) see Fitzwilliam (A. W.) and Hands. Hands, Characteristic, Oxenford (I.) is. net Apr. 12\n",
       "7795         Healey (Sir C. E. H. C.) see Dibdin (Sir L.) and Healey. Health age, Dawn of the, Moore (B.) is, net June 12\n",
       "6243                                                                     Foster (Frances G. Knowles-) see Knowles-Foster.\n",
       "18873     Yaggy (L. W.) see Haines (T. L.) and Yaggy. Yale book of American verse, Lounsbury (T. R.) Ios. net.... Dec. 12\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_to_author_entries = clean_entries.loc[\n",
    "    clean_entries.str.contains(' see ')\n",
    "]\n",
    "\n",
    "# python interprets two strings on successive lines as a single string\n",
    "# so it's easier to read long strings in source code.\n",
    "print(\n",
    "    'number of entries containing \" see \":'\n",
    "    f' {author_to_author_entries.size}\\n\\n'\n",
    ")\n",
    "\n",
    "author_to_author_entries = clean_entries.loc[\n",
    "    clean_entries.str.contains(' see ') & (lengths < 120)\n",
    "]\n",
    "author_to_author_entries.sample(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can come up with a way to identify those author-to-author lines and separate them from the entries, you'll improve our data set. That's going to be pretty difficult though, so let's start with something easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at a sample of the longest entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_entries.loc[(lengths > 325) & (lengths < 350)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A substantial number of long entries have a date that should mark a point to split entries that was missed for some reason. Reviewing a few samples in the previous cell, I see at least three cases that need to be handled:\n",
    "1. OCR mistakes, like `I2` or `1z` or `iz` rather than `12`\n",
    "2. Books published in 1911 and listed in this catalogue (our code assumes `12` is part of the entry delimiter)\n",
    "3. Entries with apparently well-formed 1912 dates that the current code failed on. For example, entry `14814` appears to be three entries separated by two 1912 dates that were skipped over for reasons I don't understand yet. Run the next cell to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_entries[14814]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two cases are relatively easy to handle. Let's start by finding the regular expressions our code uses to split entries apart and build on that to improve the current process. That code is documented in `Parsing_ECB_1912_wi23.ipynb`, currently living in the `scripts` directory of the github repository. In the section \"Splitting the Text into Entries\" in that notebook, I find the following process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL CODE IN THIS CELL IS ADAPTED FROM Parsing_ECB_1912_wi23.ipynb\n",
    "\n",
    "# read the Hathi Trust OCR into memory\n",
    "with open(f'../princeton_years/ecb_19{year}.txt', encoding='utf8') as f:\n",
    "    ocr_full_text = f.read()\n",
    "\n",
    "# Separate main text from the front matter of the catalogue (note the\n",
    "# string called ocr_main_text still has the back matter attached until\n",
    "# a later step)\n",
    "# Note python allows multiple assignment: the split operation below\n",
    "# creates a two-element list whose elements are then assigned to two\n",
    "# different variables\n",
    "ocr_front_matter, ocr_main_text = re.split(r'A\\nACADEMY', ocr_full_text)\n",
    "\n",
    "appendix_pattern = (\n",
    "    r\"APPENDIX\\nLEARNED SOCIETIES, PRINTING CLUBS, &c., \"\n",
    "    r\"WITH LISTS OF THEIR\\nPUBLICATIONS, 1912\"\n",
    ")\n",
    "\n",
    "# Separate main text from the back matter of the catalogue\n",
    "ocr_main_text, ocr_back_matter = re.split(appendix_pattern, ocr_main_text)\n",
    "\n",
    "# Make a regular expression to capture headers at the top of each\n",
    "# catalogue page\n",
    "header_capital_letters = r\"^(?:[A-Z\\-\\'\\sÈ]+)\"\n",
    "header_pattern = r\"^#(?s:.*?){}(?s:.*?){}(?s:.*?){}$\".format(\n",
    "    header_capital_letters, header_capital_letters, header_capital_letters\n",
    ")\n",
    "\n",
    "# Split the main text into pages and strip the headers. According to\n",
    "# Parsing_ECB_1912_wi23.ipynb this method fails to remove six headers.\n",
    "pages = [\n",
    "    re.sub(header_pattern, '', page, flags=re.M)\n",
    "    for page in ocr_main_text.split('\\f')\n",
    "]\n",
    "\n",
    "# Now find all lines ending in 12 and insert a token to split on\n",
    "entries_by_page = [\n",
    "    re.sub(r'(\\W12\\.?$)', '\\\\1<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in pages\n",
    "]\n",
    "\n",
    "# Split on the token to create a list of lists where each element is\n",
    "# a list of entries on an individual page\n",
    "entries_by_page = [\n",
    "    re.split(r'<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in entries_by_page\n",
    "]\n",
    "\n",
    "print(f'total entries: {sum([len(p) for p in entries_by_page])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test a new regular expression for splitting entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell assumes we're looking at the 1912 catalogue; try changing\n",
    "# this code (and the year parameter in the first code cell of the\n",
    "# notebook) to catch OCR mistakes for a different year.\n",
    "\n",
    "# create a list of possible values the OCR software might have created\n",
    "# when it ran into an 11 or 12 on the page\n",
    "ocr_interpretations_of_12 = [\n",
    "    '12',\n",
    "    'i2',\n",
    "    'I2',\n",
    "    '1z',\n",
    "    '1Z',\n",
    "    'iz',\n",
    "    'Iz',\n",
    "    'iZ',\n",
    "    'IZ'\n",
    "]\n",
    "ocr_interpretations_of_11 = [\n",
    "    '11',\n",
    "    'i1',\n",
    "    'I1',\n",
    "    '1i',\n",
    "    '1I',\n",
    "    'ii',\n",
    "    'Ii',\n",
    "    'iI',\n",
    "    'II'\n",
    "]\n",
    "terminators = ocr_interpretations_of_11 + ocr_interpretations_of_12\n",
    "\n",
    "# modify the existing regex with our new options\n",
    "entry_terminator_regex = r'(\\W({})\\.?$)'.format('|'.join(terminators))\n",
    "\n",
    "# get a new set of entries the same way as above, but using the new\n",
    "# regular expression\n",
    "new_entries_by_page = [\n",
    "    re.sub(entry_terminator_regex, '\\\\1<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in pages\n",
    "]\n",
    "new_entries_by_page = [\n",
    "    re.split(r'<ENTRY_CUT>', page, flags=re.M)\n",
    "    for page in new_entries_by_page\n",
    "]\n",
    "\n",
    "print(f'total entries: {sum([len(p) for p in new_entries_by_page])}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we might have found 157 entries that weren't caught with the previous splitting expression, meaning we might have gained 314 entries that were either not present or invalid in the previous data set. Let's see if that's really what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be much more efficient to do things like check if elements in\n",
    "# one sequence exist in another using pandas objects rather than the\n",
    "# standard python library, so I flatten the lists of lists of entries\n",
    "# into pandas series containing all the entries.\n",
    "old_entries = pd.Series(\n",
    "    [e for page in entries_by_page for e in page],\n",
    "    dtype=pd.StringDtype()\n",
    ")\n",
    "new_entries = pd.Series(\n",
    "    [e for page in new_entries_by_page for e in page],\n",
    "    dtype=pd.StringDtype()\n",
    ")\n",
    "\n",
    "# strip nonprinting characters and replace all newlines in each entry\n",
    "# with spaces\n",
    "old_entries = old_entries.str.strip()\n",
    "new_entries = new_entries.str.strip()\n",
    "old_entries = old_entries.str.replace('\\n', ' ')\n",
    "new_entries = new_entries.str.replace('\\n', ' ')\n",
    "\n",
    "# Get new entries that don't exist in the old set\n",
    "new_entries = new_entries.loc[~new_entries.isin(old_entries)]\n",
    "\n",
    "print(f'number of new entries: {len(new_entries)}')\n",
    "\n",
    "# plot a histogram of lengths of the new entries\n",
    "fig2 = new_entries.map(len).hist(bins=range(0, 400, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram we started with covers nearly 20,000 entries. Here we have only 300, but the plot generated in the previous cell already looks extremely similar to the one at the top of the document. This means we can probably assume the entries we've produced here are just as good as the rest of the entries we're already capturing. Probably! I haven't actually looked at the new entries in detail, but it's 0200 and I need to finish this up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "The goal here was to demonstrate how to play around with the data and the code for our summer project. The general method I followed was something like the following:\n",
    "1. Get some kind of overview of the output from our current parsing operations. I went with the size of the strings but there are all kinds of ways to do this. You could think about features of strings that have words in all caps and compare them to strings that don't have capitalized words (publishers in the author-first entries are almost always capitalized). You could start by exploring the dataframes, where the strings have been parsed out into specific fields, rather than thinking about whole entries. Explore the results we have and find something you think is interesting. If you can't think of a way to use that feature to explore the data, run it by one of your colleagues and see what they think. Your team is always your best asset!\n",
    "2. Use that view of the data to characterize what's working and what isn't. In this case it was clear that unreasonably short or long strings were going to have some errors, so I was able to look at those entries to see where there might be problems in the existing process.\n",
    "3. Find out which part of the existing code is relevant for the feature you're interested in. This part is probably going to be hard if you haven't had a much experience with programming. If it isn't obvious how to proceed after you've had a look at some of the scripts, ask me or Anna how to do what you want to do.\n",
    "4. See if you can modify the code to get different results.\n",
    "5. Use the same overview process you started with to compare your new result with the old one.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
